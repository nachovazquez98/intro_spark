{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este bloque es opcional y es para el caso cuando spark no puede encontrar el home de JAVA (donde esta instalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# variable de entorno JAVA_HOME, en este caso para windows (en windows la diagonal es \\\\ en linux y mac es /)\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_271\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algunos módulos importantes\n",
    "<code>pyspark.sql.functions</code> Contiene funciones internas de sql, pero pueden llamarse fuera de una string query. Más información en el siguiente \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializar el SparkContex\n",
    "La config es opcional (puede ser <b>None</b>) Este bloque solo puede ejecutarse una vez ya que solo puede existir un contexto por aplicacion (se puede reiniciar el kernel para destruir el context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear el contexto de spark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.executor.cores', '4')\n",
    "conf.set('spark.cores.max', '4')\n",
    "conf.set('spark.executor.memory', '4g')\n",
    "\n",
    "sc = pyspark.SparkContext(master=\"local\",appName=\"MyApp\",conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el archivo. No olvidar el tipo y ruta del archivo, así como el tipo de diagonal en la ruta, que es \\\\\\\\ para windows / para otros OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear sesion SQL y cargar nuestros datos\n",
    "sqlContext= pyspark.sql.SparkSession(sc)\n",
    "input_data = sqlContext.read.csv(\"D:\\\\CovidCIMAT\\\\Input\\\\ZM_2015_pob.csv\",header=True, inferSchema=True)\n",
    "input_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta linea es más que nada para \"nombrar\" la tabla que se cargo de archivo, para hacer referencia a ella más facil en queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.createOrReplaceTempView(\"zonas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query SQL, el resultado es un DataFrame\n",
    "se puede imprimir una muestra de los datos con <b>show</b> que tiene dos argumentos opcionales:\n",
    "- El número de renglones a imprimir (default 20)\n",
    "- Mostrar la información compactada (default True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\"SELECT CVE_ZM, NOM_ZM, POB_2015 from zonas\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionando columnas desde el dataframe con select\n",
    "muestro 3 formas de hacerlo (solo quita el simbolo # que es comentario):\n",
    "- con el nombre de la columna\n",
    "- con la funcion SQL COL\n",
    "- seleccionando por columna en DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = input_data.select(\"CVE_ZM\",\"NOM_ZM\",\"POB_2015\")\n",
    "#result = input_data.select(F.col(\"CVE_ZM\"),F.col(\"NOM_ZM\"),F.col(\"POB_2015\"))\n",
    "#result = input_data.select(input_data[\"CVE_ZM\"],input_data[\"NOM_ZM\"],input_data[\"POB_2015\"])\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otro ejemplo de consulta SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\"SELECT CVE_ZM, FIRST(NOM_ZM) as NOM_ZM, SUM(POB_2015) as POB from zonas group by CVE_ZM order by CVE_ZM\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misma consulta, pero usando directamente el DataFrame con funciones SQL\n",
    "Notese la función <b>alias</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = input_data.select(F.col(\"CVE_ZM\"),F.col(\"NOM_ZM\") ,F.col(\"POB_2015\")\n",
    "              ).groupBy('CVE_ZM').agg(F.first(\"NOM_ZM\").alias(\"NOM_ZM\"),F.sum(\"POB_2015\").alias(\"POB\")\n",
    "             ).orderBy('CVE_ZM')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn\n",
    "Agrega o reemplaza una columna con los datos proporcionados. Requiere 2 parámetros\n",
    "- el nombre de la columna nueva (de existir esa columna, los datos se sobreescriben, de lo contrario se agrega una nueva columna)\n",
    "- la columna de donde se copiaran los datos\n",
    "\n",
    "En este ejemplo estoy sustituyendo la columna <b>NOM_ZM</b> con los datos de la misma, pero en mayúsculas (función SQL <b>upper</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=input_data.select(\"CVE_ZM\",\"NOM_ZM\",\"POB_2015\").withColumn(\"NOM_ZM\",F.upper(\"NOM_ZM\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copiar los datos que hay en el Cluster hacia el Host\n",
    "Regresa los datos en forma de una lista de renglones <b>Row</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otra forma de recuperar datos, pero usando RDD y MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rdd.map(lambda x:(x.CVE_ZM,x.NOM_ZM,x.POB_2015)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
